---
title: 一种运用泰勒展开的网络裁剪方法
date: 2019-03-08 14:09:15
update: 2019-03-08 14:09:15
categories: [深度学习]
tags: [深度学习, 卷积网络, prune, 网络裁剪]
mathjax: true
---

深度学习模型如果想在移动端进行应用，必然面临着计算效率和模型体积的困扰，因此必须进行网络裁剪（prune）

<!-- more -->

本文方法来源于NVIDIA的一篇文章：[Pruning Convolutional Neural Networks for Resource Efficient Inference](https://arxiv.org/abs/1611.06440)

### 思路

文章的思路很明确，就是在网络模型训练完成之后，对所有的channel进行重要性度量，剔除那些对模型精度影响不大的channel，然后fine-tune。重复上述的过程，达到裁剪的目的。

![](/images/posts/dl/prune-net.jpg)

### 具体方法

我们可以将网络裁剪看作是一个优化问题：

<div align=center>
$$\underset{W'}{min}|C(D|W') - C(D|W)| \qquad s.t.||W'||_0 \leqslant  B$$
</div>

其中 $D={X={x_0, x_1,...,x_n}, Y={y_0, y_1, ..., y_n}}$ 是训练集，$W$和$W'$分别为预训练模型参数和prune之后的模型参数，作者在feature map的基础上作prune操作的，上述式子就是在prune剩余$B$个feature maps的前提下，使prune之后的参数模型在训练集的代价函数值和预训练模型在训练集的代价函数值最接近。具体步骤如下：

首先定义一个门函数，要是这个feature map需要prune的话门函数的值就为0，否则就为1，用式子表示就是：
<div align=center>
$$z_{l}^{(k)} = g_{l}^{(k)}R(z_{l-1} * w_{l}^{(k)} + b_{l}^{(k)})$$
</div>

$l$代表第$l$层卷积层，$k$表示第$k$个channel，因此，prune后的模型参数与预训练模型的参数为：
<div align=center>
$$W'=g \cdot W$$
</div>

接着就是如何判断一个channel的重要性，也就是是否需要被裁剪掉。一个channel被剪辑可以表示为：$C(D, h_{i}=0)$，$h_{i}=0$表示这个channel中的参数都为0，也就是被prune掉了。

所以我们要求的是如果一个channel被prune掉了的影响：
<div align=center>
$$\Delta |(D, h_i)|=|C(D,h_i=0)-C(D,h_i)|$$
</div>

我们知道，对于函数$C(D,h_i)$，其泰勒展开表示如下：
<div align=center>
$$C(D,h_i)=C(D,h_i=0)+\frac{\partial C}{\partial h_i}h_i-R_1(h_i=0)$$
</div>

其中，$R_1(h_i=0)$是拉格朗日余项：
<div align=center>
$$R_1(h_i=0)=\frac{\partial^2 C}{\partial (h_{i}^{2}=\varepsilon)}\frac{h_{i}^{2}}{2}$$
</div>

因为它是个很小的数，因此具体计算的时候忽略此项。

因此可得：
<div align=center>
$$\Delta |(D, h_i)|=|C(D,h_i=0)-C(D,h_i)|=|\frac{\partial C}{\partial h_i}h_i|$$
</div>

其中，$h_i$表示卷积核中各个参数，而$\frac{\partial C}{\partial h_i}$表示损失函数对参数求偏导，也就是梯度。所以，**通过计算一个卷积核的参数与其梯度的乘积，就能计算该参数对于损失函数的影响**。我们对一个channel中的参数求平均：
<div align=center>
$$\Theta_{TE}(z_{l}^{(k)})=|\frac{1}{M}\sum_{m}\frac{\partial C}{\partial z_{l,m}^{(k)}}z_{l,m}^{(k)}|$$
</div>

归一化：
<div align=center>
$$\hat{\Theta}(z_{l}^{k})=\frac{\Theta(z_{l}^{k})}{\sqrt{\sum_j(\Theta(z_{l}^j))^2}}$$
</div>

这样我们就可以得到每个channel对于损失函数的影响力。

### 实践细节

以下是一些实验过程中的实践经验：

* 在得到每个channel的影响力之后，需要判断修剪哪些channel，可以一次剔除多个，也可以只剔除一个。
* 每次裁剪之后fine-tune网络
* 当精度出现明显下降的时候，停止prune

因为泰勒展开和求解影响力引入额外的计算量，因此，一般还需要加入这部分计算量作为正则因子。
