---
title: 机器学习中关于熵的一些概念
date: 2019-12-04 15:25:37
update: 2019-12-04 15:25:37
categories: MachineLearning
tags: [机器学习, 熵, 信息增益, 基尼系数]
mathjax: true
---

交叉熵是机器学习中常用的一个概念，一般用来衡量目标值与预测值之间的差距。熵的概念源于信息论，因此，首先从信息论角度进行分析。

<!-- more -->

### 信息量

对于事件集合 $X$，其中一个随机发生的事件 $X=x_0$ 发生的概率是 $p(x_0)$，这个事件所包含的信息量为：

$$
I(x_0)-\log(p(x_0))
$$

事件发生的概率越大，信息量越小，反之，概率越小，信息量越大。这也符合人们的直观感觉。

### 信息熵

在信息论与概率统计中，熵表示随机变量不确定性的度量。熵越大，表示不确定性越大。信息熵用来表示所有信息量的期望：

$$
H(X)=-\sum_{i=1}^{n}p(x_i)\log(p(x_i))
$$

信息熵可以表示一个事件（变量）集合的不确定程度。

### 联合熵

假设要度量的对象不是单一变量的集合，而是一个联合分布的随即系统，这个系统的不确定程度可以用联合熵描述：

$$
H(X,Y)=-\sum_{x \in X} \sum_{y \in Y}p(x,y) \log p(x,y)
$$

联合熵就是联合信息量的期望。

### 条件熵

如果联合分布中有一个条件已知，和概率论中条件概率类似，联合熵就变成了条件熵：

$$
H(Y|X)=-\sum_{x \in X} \sum_{y \in Y}p(x,y) \log p(y|x)
$$

但是注意，条件熵这个定义其实是通过信息熵和联合熵推导出来的：

$$
\begin{align*}  

H(X,Y) &= -\sum_{x \in X} \sum_{y \in Y}p(x,y) \log p(x,y) \\
&= -\sum_{x \in X} \sum_{y \in Y}p(x,y) \log (p(x) \cdot p(y|x)) \\
&= -\sum_{x \in X} \sum_{y \in Y}p(x,y) \log p(x) - \sum_{x \in X} \sum_{y \in Y}p(x,y) \log p(y|x) \\
&= H(X)+H(Y|X)

\end{align*}  
$$

条件熵公式表明，对拥有两个随机变量的随机系统，我们可以先观察一个随机变量获取信息量，观察完后，我们可以在拥有这个信息量的基础上观察第二个随机变量的信息量。并且我们可以很容易推导出，不论先观察谁，对信息量都不会有影响，这是非常符合直觉的。

以上联合熵和条件熵可以进一步推广到多个随机变量系统，根据链式法则可以很容易就推导出：

$$
H(X_1,X_2,\cdots ,X_n)=\sum_{i=1}^{n}H(X_i|X_{i-1},\cdots,X_1)
$$

### 信息增益

信息增益是针对具体属性的，常用于机器学习决策树构建中的特征选择。指的是分类前的信息熵减去分类后的信息熵，即选用某个属性或者特征分类后，**信息熵的减少量**，计算如下：

$$
G(D,a)=H(D)-\sum_{v=1}^{V}\frac {|D^v|}{D}H(D^v)
$$

$H(D)$ 为划分之前的信息熵，当根据特征 $a$ 分为 $V$ 类之后，每个子类别的信息熵为 $H(D^v)$，$\frac {|D^v|}{D}$ 表示子类别数目占总数目的比例。可以看出，等号右边的后半部分就是条件熵。

注意：$H(D)$ 和 $H(D^v)$ 均看作是一个随机变量集合，在内部按照实际的类别以及比例计算各自的信息熵。这里的计算一定要明白，不懂可以看《统计学习方法》的计算实例。

信息增益用于 ID3 决策树算法中。

### 信息增益率

$$
G_R(D,a)=\frac {G(D,a)}{H(D)}
$$

信息增益率用于 C4.5 决策树算法中。

### 基尼系数

在分类问题中，假设有 $K$ 个类别，第 $k$ 个类别的概率为 $p_k$，则基尼系数的表达式为：

$$
G(p) = \sum\limits_{k=1}^{K}p_k(1-p_k) = 1- \sum\limits_{k=1}^{K}p_k^2
$$

基尼系数用于 CART 决策树算法中。

### 相对熵

相对熵又称为KL散度，常用来度量两个分部的相似程度，计算公式：

$$
D_{KL}(p||q)=\sum_{i=1}^{n}p(x_i)\log(\frac {p(x_i)} {q(x_i)})
$$

相对熵的值越小，表示一个新的分布 $q$ 和 原始分布 $p$ 越接近。相对熵用于 TensorRT 的量化校验。

### 交叉熵

$$
H(p,q)=-\sum_{i=1}^{n}p(x_i)\log(q(x_i))
$$ 

用于推导相对熵。
