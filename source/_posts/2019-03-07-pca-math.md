---
title: PCA数学理解
date: 2019-03-07 14:27:41
update: 2019-03-07 14:27:41
categories: 机器学习
tags: [机器学习, PCA, 特征向量, 特征值]
mathjax: true
---


主成分分析（PCA）在机器学习领域主要的作用就是降维，其背后有严格的数学理论支持。

<!-- more -->

考虑一组\\(M\\)个观测数据\\(\boldsymbol X\\)，每个数据具有\\(n\\)维的特征。降维的目的就是降低特征的数目，也就是将原始特征映射到\\(d < n\\)维的特征空间中。同时最大化投影数据的方差，也就是维持投影后的特征的可分性。

了解了基本含义再来看个例子，先看一个直观的图像, 下图为400个二维的数据样本，这二维的数据相关性很大。直观的感觉就是当数据投影到绿色的箭头的方向上时，方差最大。

<div align=center>
![](/images/posts/ml/pca-01.png)
</div>

换句话说，当所有点映射到一条直线时（二维变一维），这个直线的方向为绿色箭头方向的时候，可分性最好，也就是最大程度上维持了特征。我们也可以认为绿色箭头代表的一维空间是这批数据的主成分，而绿色箭头的正交方向就是我们丢弃的维度。(绿色箭头和其正交方向组成的二维空间就是图中坐标系代表的二维空间，只不过旋转了一个角度)

现在考虑将数据投射到低维的情况，也就是在\\(n\\)维的向量空间找到一个向量\\(\boldsymbol u\\)，当数据集中的数据投射到该向量上时投射后的数据集具有最大方差，实际上只需要找出向量\\(\boldsymbol u\\)的方向即可。所以可以约束向量\\(\boldsymbol u\\)的模为1，又因为向量的投影即是内积，于是数据投射到该向量上的操作就是样本的特征向量\\(\boldsymbol x_m\\)点乘向量\\(\boldsymbol u\\)。

投射后的方差为：

<div align=center>
$$\frac{1}{M}\sum_{}{}(\boldsymbol u^T \boldsymbol x_m - \boldsymbol  u^T \boldsymbol x^-)^2=\boldsymbol u^T \boldsymbol S \boldsymbol u$$
</div>

<div align=center>
$$s.t. \boldsymbol u^T \boldsymbol u=1$$
</div>

其中\\(\boldsymbol x^-\\)为观测样本集在各个维度上的均值，\\(\boldsymbol S\\)为观测样本集的协方差矩阵：

<div align=center>
$\boldsymbol S=\frac{1}{M}\sum_{}{}(\boldsymbol x_m- \boldsymbol x^-)(\boldsymbol x_m - \boldsymbol x^-)^T$
</div>

因此问题的目标就转换为在约束向量\\(\boldsymbol u\\)为单位向量的情况下，最大化\\(\boldsymbol u^T \boldsymbol S \boldsymbol u\\)。采用拉格朗日乘子法（不懂就算了）将约束优化问题转为无约束优化得到下式：

<div align=center>
$\boldsymbol u^T \boldsymbol S \boldsymbol u + \lambda (1 - \boldsymbol u^T \boldsymbol u)$
</div>

对上式中\\(\boldsymbol u\\)求偏导，然后设为0，得到：

<div align=center>
$\boldsymbol S \boldsymbol u = \lambda \boldsymbol u$
</div>

上式中\\(\boldsymbol S\\)是一个\\(n \cdot n\\)的矩阵，\\(\lambda\\)是一个标量，\\(\boldsymbol u\\)为\\(n\\)维向量。很明显，\\(\boldsymbol u\\)为矩阵\\(\boldsymbol S\\)的特征向量，表示映射后的低维特征方向，\\(\lambda\\)是对应的特征值。

以上就是从**映射求解后方差最大**这个思路来理解和推导的PCA的数学原理。

### 对角化协方差矩阵

PCA另一个数学理解角度就是对角化协方差矩阵

由于协方差矩阵的特殊性，协方差矩阵是可以对角化的，也就是存在一个矩阵\\(P\\)，满足\\(PSP^T\\)是一个对角矩阵，对角元素就是各个特征维度的权重系数，非对角元素为0，也就是

