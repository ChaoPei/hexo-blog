---
title: 决策树小结
date: 2019-03-14 14:09:15
update: 2019-03-14 14:09:15
categories: [机器学习]
tags: [机器学习, 决策树, ID3, C4.5, CART]
mathjax: true
---

决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林。

<!-- more -->

决策树的思路很简单，就是从树根开始，每次选择一个能够让样本分开的比较好的特征作为树枝的分叉点，所以其关键点在于如何衡量选择哪个特征作为分叉点是比较好的。衍生出来的主要有三种方法，下面进行一一介绍。

## 1. ID3和C4.5：通过信息熵度量

### 1.1 信息增益和ID3

**信息熵**用于度量信息的大小，其本质代表了信息的不确定性。一个随机变量$\boldsymbol X$的熵的表达式为：

$$H(\boldsymbol X) = -\sum\limits_{i=1}^{n}p_i logp_i$$

其中$n$表示$\boldsymbol X$的不同的离散取值，而$p_i$表示每种取值的概率，各种取值概率相加为1。

同理，两个变量的**联合熵**为：

$$H(\boldsymbol X, \boldsymbol Y) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i,y_i)$$

**条件熵**为：

$$H(\boldsymbol X|\boldsymbol Y) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i|y_i) = \sum\limits_{j=1}^{n}p(y_j)H(X|y_j)$$

条件熵表示在已知$\boldsymbol Y$的情况下，$\boldsymbol X$剩下的不确定性。由此我们可以得到变量$\boldsymbol Y$对于变量$\boldsymbol X$不确定性的减少程度，也就是**互信息**：

$$I(\boldsymbol X, \boldsymbol Y) = H(\boldsymbol X) - H(\boldsymbol X|\boldsymbol Y)$$

仔细一想，这不就是决策树需要的度量方式吗？是的，ID3决策树算法中就是通过这个方式来选择最优分裂的。在ID3中，互信息也称为**信息增益**

西瓜书里面举了一个选择瓜的例子，列举了计算互信息的方式，不想看书的可以看这个：[深入浅出理解决策树算法](https://zhuanlan.zhihu.com/p/26760551)


### 1.2 信息增益率和C4.5

ID3算法存在一些不足：

* ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。
* ID3采用信息增益大的特征优先建立决策树的节点。在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。
* ID3算法对于缺失值的情况没有做考虑。
* 没有考虑过拟合问题。

C4.5就是为了解决上述的问题。

对于第一个问题，C4.5的思路很简单：离散化。将连续的值量化到不同的离散区间中。比如$m$个样本的连续特征$A$有$m$个，从小到大排列为${a_1,a_2,...,a_m}$，则C4.5取相邻两样本值的平均数作为划分点，一共取得$m-1$个划分点。然后分别以这些划分点作为二元分类点进行分类来求信息增益，选择信息增益最大的点作为该连续特征的离散分类点。

对于第二个问题，引入一个新的概念：**信息增益比**，它是信息增益和特征熵的比值：

$$I_R(D,A) = \frac{I(A,D)}{H_A(D)}$$

其中$D$为样本特征输出的集合，$A$为样本特征。特征熵的表示如下：

$$H_A(D) = -\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$$

其中$n$为特征A的类别数，$D_i$为特征$A$的第i个取值对应的样本个数，其占总样本的个数就是权重比例。$D$为样本的个数。可以看到，特征数越多，其对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。

对于第三个问题，首先考虑对于这个特征的划分选择。将数据分为两部分，一部分是有这部分的特征的样本集$D_1$，一部分是没有这部分特征的样本集$D_2$，对$D_1$中各个样本计算加权（权重就是某个特征取值对应样本比例）的信息增益比，然后再乘以系数$\frac{D_1}{D_1 + D_2}$。特征划分选择完毕之后，有特征的样本可以直接划分，对于没有特征的样本，将其同时划分到所有的子节点中，同时其权重按照分配样本的数量比例来更新。

对于第四个问题，C4.5引入正则化系数进行初步的剪枝。这个后续会和CART一起讨论。

## 2. 基尼系数和CART

利用信息熵计算决策树的最优分裂虽然有效，但是对数计算的计算量比较大。可以使用另一种

